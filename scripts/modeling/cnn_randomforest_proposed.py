# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QXnL_suE2hrUXDmuiZOJf8hKXMARzw6B
"""

from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
import keras
import tensorflow as tf
from keras.layers import Input, Dense, Conv1D, MaxPooling1D, GlobalAveragePooling1D, Reshape, Flatten, BatchNormalization
from keras.optimizers import SGD

from sklearn.preprocessing import Imputer, OneHotEncoder
from sklearn import preprocessing
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.metrics import confusion_matrix

import time

import os
import os.path
from pathlib import Path

from pandas import Series
from sklearn.preprocessing import MinMaxScaler

#np.random.seed(1337)

# This script is used on Google Colab and dataset provided by Google Drive
from google.colab import drive 
drive.mount('/content/gdrive')

_dataset_folder_path = "gdrive/My Drive/filtered_all" #edit the location for your own directories


_file_names = []
_folder_locations = []  
_dataset_list = []

for dirpath, dirnames, filenames in os.walk(_dataset_folder_path): # collect all of the files with .csv extension at the given location
    for filename in [f for f in filenames if f.endswith(".csv")]:
        location = os.path.join(dirpath, filename)
        _folder_locations.append(location)
        _file_names.append(filename)

train_y = []
for i,location in enumerate(_folder_locations):
    if location.find("disgust") == -1 and location.find("happiness") == -1 and location.find("anger") == -1 and location.find("sadness") == -1:
      temp_df = pd.read_csv(location,engine='python')#, names = header_names)
      
      #normalization
      values = temp_df.iloc[:,0].values
      values = values.reshape((len(values), 1))
      scaler = MinMaxScaler(feature_range=(0, 1))
      scaler.fit(values)
      normalized = scaler.transform(values)
      temp_df = temp_df.drop(['ecg'],axis = 1)
      temp_df['ecg'] = normalized
      
      temp_df = temp_df.truncate(after = 17999) #truncate the signal
      
    
      if "fear" in location:   # label each signal with its corresponding emotion
           train_y.append(0)

      if "calmness" in location:
           train_y.append(1)
      if "anger" in location:   
           train_y.append(2)

      if "happiness" in location:
           train_y.append(3)

      if "disgust" in location:
           train_y.append(4)

      if "sadness" in location:
           train_y.append(5)
    
    
      unc_columns = ['temp','hr','spo2','timest'] # list of unwanted columns
      temp_df = temp_df.drop(unc_columns,axis=1) # drop the unwanted columns from dataset
      _dataset_list.append(temp_df)

npa = np.asarray(train_y, dtype=np.float32) #creating the labels
train_y_r = npa.reshape(312,1)

_dataset = pd.concat(_dataset_list,axis=0, sort = False)   # concatenate all dataframes which were seperated in 312 different .csv files.
_dataset.index = range(0,len(_dataset))

train_x = _dataset.iloc[:,0]
train_x = train_x.values.reshape(312,18000)

dt = np.concatenate((train_x,train_y_r),axis=1) # combine train_x with the hand made label
dt = pd.DataFrame(dt)
dt = dt.sample(frac=1).reset_index(drop=True)  # shuffle dataframe

train_x = dt.iloc[:,:-1]
train_y = dt.iloc[:,-1]

train_x = train_x.values.reshape(104,18000,1)
train_y = train_y.values.reshape(104,1)

train_y_enc = pd.DataFrame(train_y)
train_y_enc = pd.get_dummies(train_y_enc[0]) # one hot encoding for train_y

from keras import regularizers
from keras.callbacks import ReduceLROnPlateau
from sklearn.tree import DecisionTreeClassifier
from keras.models import Model
from sklearn.metrics import confusion_matrix
from keras.utils.vis_utils import plot_model
from keras.metrics import categorical_accuracy
np.random.seed(1338)


model = Sequential()
model.add(Conv1D(100,700, activation='relu',input_shape=(18000,1)))
model.add(MaxPooling1D(8))
model.add(Dropout(0.6))
model.add(BatchNormalization())
model.add(Conv1D(100,700,activation='relu'))
model.add(MaxPooling1D(6))
model.add(Dropout(0.6))
model.add(BatchNormalization())
model.add(Flatten())
model.add(Dense(16, activation='relu'))
model.add(Dropout(0.6))
model.add(Dense(2, activation = 'softmax'))
print(model.summary())
plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

rmsprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)
#sgd = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)
sgd = keras.optimizers.SGD(lr=0.0001, clipvalue=0.5)
adagrad = keras.optimizers.Adagrad(lr=0.001, epsilon=None, decay=0.0)
adam = keras.optimizers.Adam(lr=0.01, beta_1=0.99, beta_2=0.09, epsilon=None, decay=0.0, amsgrad=False)
#adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)

reduce_lr = ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.5, patience=3, min_lr=0.000001, verbose=1, cooldown=0)

model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = [categorical_accuracy])
history = model.fit(train_x,train_y_enc,epochs = 10, batch_size = 32, validation_split=0.2, shuffle=False, callbacks=[reduce_lr])
predict = model.predict(train_x)

feature_vectors_model = Model(model.input, model.layers[-3].output) # we are outputting extracted features by CNN
dtc_features = feature_vectors_model.predict(train_x) # variable holds extracted feature matrix

predict = predict.tolist()
predict_labels = []
for i in range(len(predict)):
    predict_labels.append(predict[i].index(max(predict[i])))

print("\n-------------------Predictions-------------------\n")    
print("şiddetli:", predict_labels.count(0))
print("sakin:", predict_labels.count(1))
print("üzgün:", predict_labels.count(2))
print("fear:", predict_labels.count(3))
print("happiness:", predict_labels.count(4))
print("sadness:", predict_labels.count(5))
print("\n-------------------------------------------------\n")


import matplotlib.pyplot as plt
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['categorical_accuracy'])
plt.plot(history.history['val_categorical_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# ----------------------------------- Decision Tree Classifier ------------------------------------------
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
dtc_features_normalized=(dtc_features-dtc_features.min())/(dtc_features.max()-dtc_features.min())


X_train, X_test, y_train, y_test = train_test_split(dtc_features, y_enc, test_size=0.33, random_state=31)

dtc = DecisionTreeClassifier(criterion = 'entropy',max_depth = 4, max_leaf_nodes = 5, max_features = 8)
dtc.fit(X_train,y_train)
x_pred = dtc.predict(X_train)
y_pred = dtc.predict(X_test)

cm = confusion_matrix(y_test.values.argmax(axis = 1), y_pred.argmax(axis = 1))
print(cm)

acc = accuracy_score(y_test, y_pred)

acc_train = accuracy_score(y_train, x_pred)

print("\nThe test accuracy score: ", acc)
print("\nThe training accuracy score: ", acc_train)

# ----------------------------------- Random Forest Classifier ------------------------------------------
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils.class_weight import compute_sample_weight

from sklearn.metrics import log_loss

#weight = [{0: 1, 1: 1.}, {0: 1, 1: 1.}, {0: 1, 1: 1}, {0: 1, 1: 1.}, {0: 1, 1: 1.}, {0: 1, 1: 1.}]
#sample_w = compute_sample_weight(weight, y_train)

# [{1:1}, {2:5}, {3:1}, {4:1}].

clf = RandomForestClassifier(n_estimators=1000, max_depth=3, random_state=11,  max_features = 4,max_leaf_nodes = 3)#, class_weight = weight)
clf.fit(X_train, y_train)#, sample_weight = sample_w)

x_pred = clf.predict(X_train)
y_pred = clf.predict(X_test)

cm = confusion_matrix(y_test.values.argmax(axis = 1), y_pred.argmax(axis = 1))
print(cm)

acc = accuracy_score(y_test, y_pred)

acc_train = accuracy_score(y_train, x_pred)

test_loss = log_loss(y_test, y_pred)
train_loss = log_loss(y_train,x_pred)

print("\nThe test loss :",test_loss)
print("The test accuracy score: ", acc)
print("\nThe train loss :",train_loss)
print("The training accuracy score: ", acc_train)

# ----------------------------------- Export the feature matrix ------------------------------------------
b = train_y_enc.values.argmax(axis = 1)
b = b.reshape(312,1)
a  = np.concatenate((dtc_features,b), axis=1)

np.savetxt("extracted_features_6_upd.csv", a, delimiter=",")